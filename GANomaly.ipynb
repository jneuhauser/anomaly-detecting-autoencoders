{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training\n",
    "Paper: https://arxiv.org/abs/1805.06725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import models.ganomaly as mg\n",
    "import datasets.common as cds\n",
    "import datasets.mvtec_ad as mvds\n",
    "import utils.datasets as dsu\n",
    "import utils.plot as pu\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mod in (mg, cds, dsu, pu):\n",
    "    reload(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Legacy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = dsu.create_anomaly_dataset(\n",
    "    cds.get_dataset('mnist'),\n",
    "    abnormal_class=2\n",
    ")\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n",
    "#print(train_labels[:10])\n",
    "#print(test_labels[5395:5405])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_images.shape[1] > 64:\n",
    "    train_images = tf.image.resize(train_images, (64,64))\n",
    "    test_images = tf.image.resize(test_images, (64,64))\n",
    "elif train_images.shape[1] not in [2**x for x in range(10)]:\n",
    "    power = 1\n",
    "    while power < train_images.shape[1]:\n",
    "        power *= 2\n",
    "    new_size = (power, power)\n",
    "    print(\"resizing to:\", new_size)\n",
    "    train_images = tf.image.resize(train_images, new_size)\n",
    "    test_images = tf.image.resize(test_images, new_size)\n",
    "train_labels = train_labels.reshape((-1,1))\n",
    "test_labels = test_labels.reshape((-1,1))\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abnormal_start = dsu.find_abnormal_start_index(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.plot_images(test_images[abnormal_start-5:abnormal_start+5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVTec AD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'bottle', 'cable', 'capsule', 'carpet',\n",
    "    'grid', 'hazelnut', 'leather', 'metal_nut',\n",
    "    'pill', 'screw', 'tile', 'toothbrush',\n",
    "    'transistor', 'wood', 'zipper'\n",
    "]\n",
    "\n",
    "category = 0\n",
    "channels = 1\n",
    "resolution = 64\n",
    "buffer_size = 1000\n",
    "\n",
    "resize_image = lambda image, label: (tf.image.resize(image, (resolution, resolution)), label)\n",
    "\n",
    "test_ds = mvds.get_labeled_dataset(\n",
    "    category=categories[category],\n",
    "    split = 'test',\n",
    "    image_channels=channels,\n",
    "    binary_labels=True\n",
    ")\n",
    "test_ds = test_ds.map(resize_image, num_parallel_calls=AUTOTUNE)\n",
    "test_ds = test_ds.cache('/tmp/tfdata_test_ds.cache')\n",
    "\n",
    "train_ds = mvds.get_labeled_dataset(\n",
    "    category=categories[category],\n",
    "    split = 'train',\n",
    "    image_channels=channels,\n",
    "    binary_labels=True\n",
    ")\n",
    "train_ds = train_ds.map(resize_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.cache('/tmp/tfdata_train_ds.cache')\n",
    "train_ds = train_ds.repeat(10)\n",
    "train_ds = train_ds.shuffle(buffer_size)\n",
    "train_ds = train_ds.prefetch(buffer_size)\n",
    "\n",
    "def augment_image(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    # Add 6 pixels of padding\n",
    "    image = tf.image.resize_with_crop_or_pad(image, resolution + 6, resolution + 6) \n",
    "    # Random crop back to the original size\n",
    "    image = tf.image.random_crop(image, size=[resolution, resolution, channels])\n",
    "    #image = tf.image.random_brightness(image, max_delta=0.5)\n",
    "    #image = tf.clip_by_value(image, 0, 1)\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(augment_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "train_count = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "test_count = tf.data.experimental.cardinality(test_ds).numpy()\n",
    "print(\"train_count: {}, test_count: {}\".format(train_count, test_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mg)\n",
    "model = mg.GANomaly(\n",
    "    #input_shape=train_images[0].shape,\n",
    "    input_shape=(resolution, resolution, channels),\n",
    "    latent_size=300\n",
    ")\n",
    "model.compile(metrics=[tf.keras.metrics.AUC()])\n",
    "#model.build((None, *train_images[0].shape))\n",
    "model.build((None, resolution, resolution, channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(mg)\n",
    "#adcb = mg.ADModelEvaluator(test_images.shape[0])\n",
    "adcb = mg.ADModelEvaluator(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)\n",
    "batch_size = 128\n",
    "results = model.fit(\n",
    "    #x=train_images,\n",
    "    #y=train_labels,\n",
    "    x=train_ds.batch(batch_size),\n",
    "    #batch_size=batch_size,\n",
    "    epochs=100,\n",
    "    #validation_data=(test_images, test_labels),\n",
    "    #validation_batch_size=test_labels.shape[0]//10,\n",
    "    validation_data=test_ds.batch(batch_size),\n",
    "    callbacks=[adcb],\n",
    "    #verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(adcb.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.history.keys())\n",
    "\n",
    "plt.plot(adcb.test_results)\n",
    "plt.title('test results')\n",
    "plt.ylabel('AUC (ROC)')\n",
    "plt.xlabel('epoch')\n",
    "#plt.legend(['generator'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(results.history['loss_gen'])\n",
    "plt.title('generator loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['generator'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(results.history['loss_gen_adv'])\n",
    "plt.plot(results.history['loss_gen_rec'])\n",
    "plt.plot(results.history['loss_gen_enc'])\n",
    "plt.title('generator specific losses')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['adversarial', 'reconstruction', 'encoder'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(results.history['loss_dis'])\n",
    "plt.plot(results.history['loss_dis_real'])\n",
    "plt.plot(results.history['loss_dis_fake'])\n",
    "plt.title('discriminator losses')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['real/fake', 'real', 'fake'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# makes no sense as this is only the per image mse between latent_i and latent_o\n",
    "tf.config.run_functions_eagerly(True)\n",
    "eval_results = model.evaluate(\n",
    "    x=test_images,\n",
    "    y=test_labels,\n",
    "    batch_size=test_images.shape[0]//10,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(\n",
    "    x=test_images,\n",
    "    batch_size=test_images.shape[0]//10\n",
    ")\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = np.min(predictions)\n",
    "ptp_val = np.ptp(predictions)\n",
    "print(\"ptp_val:\", ptp_val, \"min_val:\", min_val)\n",
    "\n",
    "predictions -= min_val\n",
    "predictions /= ptp_val\n",
    "\n",
    "print(predictions[abnormal_start-15:abnormal_start+15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels[abnormal_start-5:abnormal_start+5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_normal = model.predict(\n",
    "    x=test_images[:abnormal_start],\n",
    "    batch_size=test_images[:abnormal_start].shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max:\", np.max(predictions_normal))\n",
    "print(\"min:\", np.min(predictions_normal))\n",
    "print(\"mean:\", np.mean(predictions_normal))\n",
    "print(\"q(50):\", np.percentile(predictions_normal, 50))\n",
    "print(\"q(75):\", np.percentile(predictions_normal, 75))\n",
    "print(\"q(90):\", np.percentile(predictions_normal, 90))\n",
    "print(\"q(95):\", np.percentile(predictions_normal, 95))\n",
    "print(\"q(99):\", np.percentile(predictions_normal, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_abnormal = model.predict(\n",
    "    x=test_images[abnormal_start:],\n",
    "    batch_size=test_images[abnormal_start:].shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max:\", np.max(predictions_abnormal))\n",
    "print(\"min:\", np.min(predictions_abnormal))\n",
    "print(\"mean:\", np.mean(predictions_abnormal))\n",
    "print(\"q(50):\", np.percentile(predictions_abnormal, 50))\n",
    "print(\"q(75):\", np.percentile(predictions_abnormal, 75))\n",
    "print(\"q(90):\", np.percentile(predictions_abnormal, 90))\n",
    "print(\"q(95):\", np.percentile(predictions_abnormal, 95))\n",
    "print(\"q(99):\", np.percentile(predictions_abnormal, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(reconstructed, latent_i, latent_o), (classifier, features) = model(test_images[abnormal_start-5:abnormal_start+5], training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image_tuples = list(zip(\n",
    "    test_images[abnormal_start-5:abnormal_start+5],\n",
    "    reconstructed\n",
    "))\n",
    "p.plot_image_tuples(image_tuples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
